{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailog_text = \"研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/dailog.txt\"\n",
    "dir_text = \"/研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/\"\n",
    "dir_q_t = \"/研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/question/train\"\n",
    "dir_q_v = \"/研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/question/val\"\n",
    "dir_a_t = \"/研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/answer/train\"\n",
    "dir_a_v = \"/研究/対話破綻/projectnextnlp-chat-dialogue-corpus/json/answer/val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレーニングデータと検証データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = [], []\n",
    "file_path = \"\"\n",
    "for line in open(dailog_text, 'r'):\n",
    "    idx = line.find('_')\n",
    "    questions.append(line[:idx])\n",
    "    answers.append(line[idx+1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utterance = questions[0:len(questions)-int(len(questions)*0.50)]\n",
    "val_utterance = questions[int(len(questions)*0.50):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response = answers[0:len(answers)-int(len(answers)*0.50)]\n",
    "val_response = answers[int(len(answers)*0.50):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークンを分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(wakati=True)\n",
    "inp_tokens, out_tokens = [], []\n",
    "for i in questions:\n",
    "    inp_tokens += tokenizer.tokenize(i)\n",
    "for j in answers:\n",
    "    out_tokens += tokenizer.tokenize(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_vocab_lists = list(set(inp_tokens))\n",
    "out_vocab_lists = list(set(out_tokens))\n",
    "tokenizaer_utteranse = tfds.features.text.TokenTextEncoder(inp_vocab_lists, tokenizer=tokenizer)\n",
    "tokenizaer_response = tfds.features.text.TokenTextEncoder(out_vocab_lists, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データをスライス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_utterance, train_response))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_utterance, val_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを学習できる形に持っていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1,lang2):\n",
    "    lang1 = [tokenizaer_utteranse.vocab_size] + tokenizaer_utteranse.encode(lang1.numpy()) + [tokenizaer_utteranse.vocab_size + 1]\n",
    "    lang2 = [tokenizaer_response.vocab_size] + tokenizaer_response.encode(lang1.numpy()) + [tokenizaer_response.vocab_size + 1]\n",
    "    return lang1,lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encoder(ut, re):\n",
    "    return tf.py_function(encode, [ut,re], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, ?), (?, ?)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_dataset.map(tf_encoder)\n",
    "train_dataset = train_data.filter(filter_max_length)\n",
    "train_data = train_data.cache()\n",
    "train_data = train_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_data.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
